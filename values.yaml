# Default values for zoochacha-manifest.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# This will set the replicaset count more information can be found here: https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/
replicaCount: 1

# This sets the container image more information can be found here: https://kubernetes.io/docs/concepts/containers/images/
image:
  repository: nginx
  # This sets the pull policy for images.
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "zoochacha-nginx"

# This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: "zoochacha"
fullnameOverride: "zoochacha"

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# This is for setting Kubernetes Annotations to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
podAnnotations: {}
# This is for setting Kubernetes Labels to a Pod.
# For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
podLabels: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

# This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
service:
  # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
  type: ClusterIP
  # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
  port: 80

# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false

resources:
  nginx:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  admin:
    requests:
      cpu: 200m
      memory: 256Mi
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

# This is to setup the liveness and readiness probes more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  httpGet:
    path: /
    port: http
readinessProbe:
  httpGet:
    path: /
    port: http

# This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# Global Values
global:
  environment: production
  clusterName: zoochacha-eks-cluster
  domain: zoochacha.com
  adminDomain: www.zoochacha.com/admin
  # Namespace 설정
  namespaces:
    cert_manager: cert-manager
    argocd: argocd
    nginx: zoochacha
    db: zoochacha-db

# Namespaces Configuration
namespaces:
  - name: zoochacha
    labels:
      app.kubernetes.io/part-of: zoochacha
  - name: zoochacha-db
    labels:
      app.kubernetes.io/part-of: zoochacha
  - name: cert-manager
    labels:
      app.kubernetes.io/part-of: zoochacha
  - name: argocd
    labels:
      app.kubernetes.io/part-of: zoochacha

# AWS 설정
aws:
  region: ap-northeast-2
  # 민감정보는 secrets.yaml에서 가져옴
  # accessKeyID, secretAccessKey, roleArn

# Cert-Manager Configuration
certmanager:
  enabled: false
  installCRDs: false
  webhook:
    timeoutSeconds: 30
  namespace: cert-manager
  
  # DNS 챌린지를 위한 솔버 설정
  solver:
    dns01:
      route53:
        region: ap-northeast-2
        accessKeyID: ""
        secretAccessKey: ""
        hostedZoneID: ""

# Nginx Ingress Configuration
nginx-ingress:
  enabled: false

# Nginx Proxy Configuration
nginxProxy:
  replicaCount: 1
  image:
    repository: nginx
    tag: "1.25.3"
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
    port: 80
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  configMap:
    data:
      nginx.conf: |
        worker_processes auto;
        events {
          worker_connections 1024;
        }
        http {
          upstream map_backend {
            server map-server.zoochacha.svc.cluster.local:8002;
          }
          upstream admin_backend {
            server admin-server.zoochacha.svc.cluster.local:8001;
          }
          server {
            listen 80;
            server_name _;
            
            location / {
              proxy_pass http://map_backend;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
            }
            
            location /admin {
              proxy_pass http://admin_backend;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
            }
          }
        }

# Nginx Configuration
nginx:
  enabled: true
  image:
    repository: nginx
    tag: "1.25.3"
    pullPolicy: IfNotPresent
  service:
    type: LoadBalancer
  serverBlock: |-
    server {
      listen 0.0.0.0:8080;
      
      location / {
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 요청을 해당 서비스로 전달
        proxy_pass http://map-server.zoochacha.svc.cluster.local:8002;
      }
      
      location /admin {
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # 요청을 관리자 서비스로 전달
        proxy_pass http://admin-server.zoochacha.svc.cluster.local:8001;
      }
      
      # 다른 서비스 경로 설정
      {{- range .Values.locations }}
      location {{ .path }} {
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        
        {{- if eq .service "map" }}
        proxy_pass http://map-server.zoochacha.svc.cluster.local:8002;
        {{- else if eq .service "flask_admin" }}
        proxy_pass http://admin-server.zoochacha.svc.cluster.local:8001;
        {{- else if eq .service "reservation" }}
        proxy_pass http://reservation-server.zoochacha.svc.cluster.local:8003;
        {{- else if eq .service "reservation_detail" }}
        proxy_pass http://rds-server.zoochacha.svc.cluster.local:8004;
        {{- end }}
        
        {{- if .buffering }}
        proxy_buffering on;
        {{- else }}
        proxy_buffering off;
        {{- end }}
        
        {{- if .caching }}
        expires {{ .caching.expires }};
        add_header Cache-Control "{{ .caching.cacheControl }}";
        {{- if .caching.pragma }}
        add_header Pragma "{{ .caching.pragma }}";
        {{- end }}
        {{- end }}
      }
      {{- end }}
      
      # 헬스 체크 엔드포인트
      location /healthz {
        return 200 "healthy\n";
      }
    }

# Services Configuration
services:
  - name: flask_admin
    serviceName: admin-server
    port: 8001
  - name: map
    serviceName: map-server
    port: 8002
  - name: reservation
    serviceName: reservation-server
    port: 8003
  - name: reservation_detail
    serviceName: rds-server
    port: 8004

# Location Configuration
locations:
  - path: /
    service: map
    buffering: true
  - path: /parking-lot
    service: reservation
  - path: ~ ^/parking-lot/(\d+)$
    service: reservation
  - path: ~ ^/reservation/(\d+)/reserve$
    service: reservation
  - path: ~ ^/reservation-detail/(\d+)$
    service: reservation_detail
  - path: ~ ^/reservation-detail/api/detail/(\d+)$
    service: reservation_detail
  - path: ~ ^/reservation-detail/reactivate/(\d+)$
    service: reservation_detail
  - path: /reservation-detail/not-found
    service: reservation_detail
  - path: /admin
    service: flask_admin
  - path: /login
    service: flask_admin
  - path: /role_check
    service: flask_admin
  - path: /authorize
    service: flask_admin
  - path: /callback
    service: flask_admin
  - path: /logout
    service: flask_admin
  - path: ~ ^/admin/reservation/(\d+)$
    service: flask_admin
  - path: ~ ^/admin/parkinglot/edit/(\d+)$
    service: flask_admin
  - path: /admin/parkinglot
    service: flask_admin
  - path: /static/
    service: map
    buffering: true
    caching:
      expires: -1
      cacheControl: "no-cache, no-store, must-revalidate"
      pragma: "no-cache"
      expireTime: 0
  - path: /reservation/static/
    service: reservation
    caching:
      expires: 7d
      cacheControl: "public, max-age=604800"

# Database Configuration
database:
  enabled: true
  type: mysql
  host: "mysql-0.mysql.zoochacha-db.svc.cluster.local"
  port: 3306
  database: zoochacha
  replicas: 3
  image:
    repository: mysql
    tag: "5.7"
  xtrabackup:
    repository: gcr.io/google-samples/xtrabackup
    tag: "1.0"
  service:
    port: 3306
  storageClass:
    name: mysql-retain-sc
    provisioner: kubernetes.io/aws-ebs
    type: gp2
    reclaimPolicy: Retain
    allowVolumeExpansion: true
  storage:
    size: 10Gi
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1000m
      memory: 2Gi
  # Sensitive values in secrets.yaml:
  # username, password, rootPassword

# ArgoCD Configuration
argocd:
  enabled: false
  application:
    enabled: false
    source:
      targetRevision: main
    destination:
      server: https://kubernetes.default.svc

# TLS Configuration
tls:
  enabled: true
  clusterIssuer:
    name: letsencrypt-prod
    email: admin@zoochacha.com

# ArgoCD Chart 설정 (서브차트)
argo-cd:
  enabled: true
  global:
    domain: argocd.zoochacha.com
  installCRDs: true
  namespace: argocd
  server:
    extraArgs:
      - --insecure
    ingress:
      enabled: false  # 우리가 직접 Ingress를 관리할 것이므로 비활성화
    service:
      type: ClusterIP  # Ingress를 통해 접근할 것이므로 ClusterIP로 변경
  controller:
    enableStatefulSet: true
  dex:
    enabled: false
  notifications:
    enabled: true
  applicationSet:
    enabled: true

# 민감정보 - Git에 포함되지 않는 파일
secrets:
  aws:
    accessKeyID: "" # 실제 값은 secrets.yaml 파일에서 제공
    secretAccessKey: "" # 실제 값은 secrets.yaml 파일에서 제공
    roleArn: "" # 실제 값은 secrets.yaml 파일에서 제공
  database:
    username: "" # 실제 값은 secrets.yaml 파일에서 제공
    password: "" # 실제 값은 secrets.yaml 파일에서 제공
    rootPassword: "" # 실제 값은 secrets.yaml 파일에서 제공
  argocd:
    githubToken: "" # 실제 값은 secrets.yaml 파일에서 제공
    repositories:
      - name: admin-service
        url: https://github.com/Goorm-Cloud/manifast-admin.git
      - name: map-service
        url: https://github.com/Goorm-Cloud/manifast-map.git
      - name: reservation-service
        url: https://github.com/Goorm-Cloud/manifast-reservation.git
      - name: reservation-detail-service
        url: https://github.com/Goorm-Cloud/manifast-rds.git

# HPA 설정
hpa:
  admin:
    enabled: true
    minReplicas: 2
    maxReplicas: 4
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  map:
    enabled: true
    minReplicas: 3
    maxReplicas: 15
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  reservation:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  rds:
    enabled: true
    minReplicas: 2
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

# 각 서비스의 리소스 설정
resources:
  admin:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi
  map:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      cpu: 700m
      memory: 1Gi
  reservation:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      cpu: 700m
      memory: 1Gi
  rds:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Karpenter Configuration
karpenter:
  enabled: false
  installCRDs: false
  webhook:
    timeoutSeconds: 30
  serviceAccount:
    roleArn: "arn:aws:iam::651706756261:role/KarpenterControllerRole-zoochacha"

# RBAC Configuration
rbac:
  create: true
  serviceAccountName: zoochacha-installer
  namespace: zoochacha
  rules:
    - apiGroups: [""]
      resources: ["secrets", "configmaps", "services"]
      verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
    - apiGroups: ["apps"]
      resources: ["deployments", "statefulsets"]
      verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
    - apiGroups: ["batch"]
      resources: ["jobs"]
      verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Hooks Configuration
hooks:
  enabled: true
  preInstall:
    enabled: true
    weight: -100  # 가장 먼저 실행
    jobs:
      - name: create-namespaces
        weight: -100
      - name: install-cert-manager-crds
        weight: -90
      - name: install-argocd-crds
        weight: -80
      - name: install-karpenter-crds
        weight: -70
  postInstall:
    enabled: true
    weight: 100
    jobs:
      - name: wait-for-cert-manager
        weight: 10
      - name: wait-for-argocd
        weight: 20
      - name: wait-for-karpenter
        weight: 30

# ArgoCD Configuration
argo:
  enabled: true
  namespace: argocd
  domain: argo.zoochacha.online
  server:
    service:
      type: ClusterIP

# Karpenter Configuration
karpenter:
  enabled: false
  namespace: karpenter
